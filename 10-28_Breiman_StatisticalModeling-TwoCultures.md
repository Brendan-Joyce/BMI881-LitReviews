# 10/28 - Breiman ~ Statistical Modeling: the Two Cultures
## Literature Review:
Dr. Breiman’s review of modern statistical approaches, as of 2001, in “Statistical Modeling: the Two Cultures” underlines the importance of his understanding of the evolving data science field. He identifies two specific classes of data science, one grounded in traditional statistical modeling, and the other more grounded in empirical “algorithmic” methods, like decision trees and neural networks. Shaped by a lifetime of experience in the field of data science and probabilistic modeling, Breiman asserts that the assumptions often made by individuals in the data modeling camp are violated by the very models they design, thereby harming the value of inferences that can be drawn from them. He then describes a growing class of algorithmic modeling opportunities, lauding their accuracy and precision in comparison to the traditional statistical approaches. His algorithmic models represent those whose structure cannot be explained in a simplified equation form but rather adopt layered paradigms. He concludes that “algorithmic” models should be more common place than data models by retroactively considering the failures of prior projects where he developed data models as a contractor in the context of recent successes from projects he’s analyzed since returning to academia. Ultimately, he weighs the interpretability of novel algorithmic models against that of the more well understood data models to assert his reasoning that algorithmic models necessitate further adoption and exploration.


I find Breiman’s work to be well reported and thoughtfully constructed, though I believe that some of the issues he brings are more representative of opinions regarding traditional data modeling methods rather than objective problems. With that said, I don’t disagree with his opinions, my own history in data modeling followed a very similar path to his, with my initial focus originally starting on probabilistic modeling of EHR data. Part of the reason I returned for my PhD is echoed in many of the problems he brings up. During my time in data driven modeling, I felt as though many of the approaches we used failed to actively address the biases and limitations of our datasets especially when addressing hidden or unmeasured confounders. Like Breiman, I saw that many of the models we had trained relied on assumptions of linearity and inherently unstable parameter winnowing methods. Breiman’s acknowledgement of those problems allows him to leverage the rest of his narrative to describe why he believes that “algorithmic” methods could offer a new capacity to address many of these shortcomings. Even with a very primordial landscape of deep modeling available at the time, Breiman’s excitement and interest in the modeling space reads clearly as not only a critique of the status quo, but also an invitation to other researchers to explore the burgeoning field of deep modeling. 

## Questions:
1.	Now that modeling through deep networks has taken center stage in research communities today, its limitations and assumptions have been made more clear. How do you think Breiman would respond to critics of deep modeling, specifically those who believe that these models produce a “Tyranny of Empiricism”?
2.	Building on my prior question, what has been the main role of statisticians in data science research as we move further away from mathematical probabilistic modeling? 
3.	Following the widespread adoption of more complex modeling techniques, statistics grounded approaches were developed to address shortcomings in feature relevance/importance. Do you think that Breiman would appreciate the development of methods like SHAPP or LIME to empirically enhance explainability of these complex deep models?

Breiman, Leo. "Statistical modeling: The two cultures (with comments and a rejoinder by the author)." Statistical science 16.3 (2001): 199-231.
