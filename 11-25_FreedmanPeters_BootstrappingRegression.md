# 11/25 ~ David Freedman and Steven Peters ~ Bootstrapping a Regression Equation: Some Empirical Results
## Literature Review
David Freedman and Steven Peter’s paper “Bootstrapping a Regression Equation: Some Empirical Results” is a key paper in describing how and why the bootstrapped regression method is effective for improving predictive accuracy of model estimators. Their work addresses the asymptotic gap between finite sampling and the assumptions that underpin the standard regression method for calculating model coefficients through the application of the Monte Carlo bootstrapping regression method. Pre-existing regression methods like original least squares rely on a multivariate normal distribution of error parameters resulting in biased estimation when error values do not distribute normally and sampling is not large enough as to accommodate the central limit theorem. Through their mathematical analysis they assert that resampling the residuals from the fitted model numerous times allows for the estimation of weight distributions free from parametric constraints. To validate these findings, they establish a set of mathematical proofs regarding the coefficient estimation procedures and analyze a preexisting econometrics dataset showing that their predicted variance better lines up with reality.

Overall, despite the simplicity of their proofs and the underlying procedures, I find this paper rather dense and difficult to read. I have prior experience in applying and studying the Monte Carlo bootstrapping regression method and this paper still required 2-3 readings to really grip. Ultimately, I think it comes down to the organization of the narrative and the way they discuss their experiments and the procedures itself. First, the procedure’s algorithm is largely explained in a paragraph of text with a  few interspersed equations, not pseudocode. This can make it difficult to resolve the actual steps of the procedure on first read. Secondly, the figures provided to represent their findings are largely tables, making the results far more difficult to understand in the context of its reported successes. This is all to say, that while I find the overall science to be solid in this paper, I think the mode of presentation for the paper damages its ability to reach the broader audience of individuals who would apply it.

## Questions
1.	We’ve read a handful of statistical procedure papers in the past few weeks, and yet this one has by far and away the fewest number of citation, ~650, despite representing a method frequently used today. Why is it that this specific paper is considerably less popular?
2.	I’ve heard of other bootstrapping methods, wherein X, Y pairs are resampled with replacement to train numerous regression models and then the average of those coefficients are used to establish the distribution of coefficient values. Those methods feel less refined than the one’s reported in this paper. Are they older versions of Monte Carlo approaches, common industry hacks to get around a more computationally expensive problem, or just poor game of telephone when trying to describe the traditional bootstrapping method?
3.	How do coefficients established from cross fold validation compare to those established through this resampling based approach?

Freedman, David A., and Stephen C. Peters. "Bootstrapping a regression equation: Some empirical results." Journal of the American Statistical Association 79.385 (1984): 97-106.
